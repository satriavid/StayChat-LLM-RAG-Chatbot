{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain openai chromadb\n",
        "!pip install -q langchain-community\n",
        "!pip install -q tiktoken"
      ],
      "metadata": {
        "id": "JGXR0-MsaE1T"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-xxx\""
      ],
      "metadata": {
        "id": "Zmk3KQMqZvsO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Sample Data"
      ],
      "metadata": {
        "id": "dCpLAw4RH-dh"
      }
    },
    {
      "source": [
        "!pip install -q wget==3.2"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Zq-mHUnwJSwN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: download this file from https://github.com/satriavid/StayChat-LLM-RAG-Chatbot/blob/main/data/uk_final_df.csv and put into /content\n",
        "\n",
        "import os\n",
        "import wget\n",
        "\n",
        "url = \"https://github.com/satriavid/StayChat-LLM-RAG-Chatbot/blob/main/data/uk_final_df.csv\"\n",
        "filename = os.path.basename(url)\n",
        "\n",
        "wget.download(url, out=filename)\n",
        "\n",
        "# Move the downloaded file to the desired location\n",
        "#!mv {filename} /content\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "AdUQ8ZvdI4kG",
        "outputId": "8eeaae10-f2f9-4a53-f682-ccd1350338cd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'uk_final_df.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Deployment\n",
        "Streamlit as Frontend and NGROK as tunnel for accessing from public URL"
      ],
      "metadata": {
        "id": "IWgtls4nJs1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit pyngrok"
      ],
      "metadata": {
        "id": "8VCHAIJ6AXkO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('ai.py', 'w') as f:\n",
        "    f.write(\"\"\"\n",
        "from langchain.document_loaders import CSVLoader\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import OpenAI\n",
        "from langchain_community.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "loader = CSVLoader(file_path='/content/uk_final_df.csv')\n",
        "\n",
        "# Create an index using the loaded documents\n",
        "embedding = OpenAIEmbeddings()\n",
        "index_creator = VectorstoreIndexCreator(embedding=embedding)\n",
        "docsearch = index_creator.from_loaders([loader])\n",
        "\n",
        "# Create a question-answering chain using the index\n",
        "chain = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=docsearch.vectorstore.as_retriever(), input_key=\"question\")\n",
        "\n",
        "def get_ai_response(query):\n",
        "    response = chain({\"question\": query})\n",
        "    return response['result']\n",
        "\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "qd4CnIV6BAv5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('app.py', 'w') as f:\n",
        "    f.write(\"\"\"\n",
        "import streamlit as st\n",
        "import ai\n",
        "\n",
        "st.title(\"StayChat by NLP-B Ilya\")\n",
        "\n",
        "# Initialize chat history in session state\n",
        "if 'messages' not in st.session_state:\n",
        "    st.session_state['messages'] = [{\"role\": \"assistant\", \"content\": \"Hello, How can I help you Today?\"}]\n",
        "\n",
        "# Display chat messages from history\n",
        "for message in st.session_state['messages']:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "# Accept user input and display it\n",
        "if prompt := st.chat_input(\"You:\"):\n",
        "    st.session_state['messages'].append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "\n",
        "    # Generate and display assistant's response\n",
        "    response = ai.get_ai_response(prompt)\n",
        "    st.session_state['messages'].append({\"role\": \"assistant\", \"content\": response})\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        st.markdown(response)\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "b5nECOMl_76D"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Streamlit Server\n",
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "\n",
        "# Set up ngrok with your authtoken\n",
        "authtoken = \"\"\n",
        "ngrok.set_auth_token(authtoken)\n",
        "\n",
        "# Start the Streamlit server\n",
        "process = subprocess.Popen(['streamlit', 'run', 'app.py'])\n",
        "\n",
        "# Create an ngrok tunnel to the Streamlit server\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"Streamlit app is live at: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiXPkNKP_26a",
        "outputId": "d1d4bbd4-0bde-4ffe-9908-4aadab42528a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app is live at: NgrokTunnel: \"https://eea7-35-231-23-41.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Kill Streamlit and NGROK\n",
        "\n",
        "import os\n",
        "import signal\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Kill the Streamlit server process\n",
        "if 'process' in globals():\n",
        "    os.kill(process.pid, signal.SIGTERM)\n",
        "\n",
        "# Disconnect ngrok\n",
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "yJlRUovaBlsD"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}